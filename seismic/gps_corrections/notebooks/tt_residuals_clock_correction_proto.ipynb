{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.width', 240)\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pytz\n",
    "import matplotlib\n",
    "import matplotlib.dates\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = (16.0, 9.0)\n",
    "matplotlib.rcParams['figure.max_open_warning'] = 100\n",
    "matplotlib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress bar helper to indicate that slow tasks have not stalled\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PICKS_PATH = r\"C:\\data_cache\\Picks\\20190320\\ensemble.p.txt\"\n",
    "#PICKS_PATH = r\"C:\\data_cache\\Picks\\20190219\\ensemble_small.p.txt\"\n",
    "dtype = {'#eventID': object,\n",
    "    'originTimestamp': np.float64,\n",
    "    'mag':                    np.float64,\n",
    "    'originLon':              np.float64,\n",
    "    'originLat':              np.float64,\n",
    "    'originDepthKm':          np.float64,\n",
    "    'net':                     object,\n",
    "    'sta':                     object,\n",
    "    'cha':                     object,\n",
    "    'pickTimestamp':          np.float64,\n",
    "    'phase':                   object,\n",
    "    'stationLon':             np.float64,\n",
    "    'stationLat':             np.float64,\n",
    "    'az':                     np.float64,\n",
    "    'baz':                    np.float64,\n",
    "    'distance':               np.float64,\n",
    "    'ttResidual':             np.float64,\n",
    "    'snr':                    np.float64,\n",
    "    'qualityMeasureCWT':      np.float64,\n",
    "    'domFreq':                np.float64,\n",
    "    'qualityMeasureSlope':    np.float64,\n",
    "    'bandIndex':              np.int64,\n",
    "    'nSigma':                 np.int64}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utc_time_string_to_plottable_datetime(utc_timestamp_str):\n",
    "    \"\"\"\n",
    "    Convert a UTC timestamp string to datetime type that is plottable by matplotlib\n",
    "\n",
    "    :param utc_timestamp_str: ISO-8601 UTC timestamp string\n",
    "    :type utc_timestamp_str: str\n",
    "    :return: Plottable datetime value\n",
    "    :rtype: datetime.datetime with tzinfo\n",
    "    \"\"\"\n",
    "    utc_time = obspy.UTCDateTime(utc_timestamp_str)\n",
    "    return pytz.utc.localize(datetime.datetime.utcfromtimestamp(float(utc_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMP_DEPLOYMENTS = {}\n",
    "TEMP_DEPLOYMENTS['7X'] = (utc_time_string_to_plottable_datetime('2009-06-16T03:42:00.000000Z'),\n",
    "                          utc_time_string_to_plottable_datetime('2011-04-01T23:18:49.000000Z'),\n",
    "                          'C7', 'Deployment 7X')\n",
    "TEMP_DEPLOYMENTS['7D'] = (utc_time_string_to_plottable_datetime('2012-01-01T00:01:36.000000Z'),\n",
    "                          utc_time_string_to_plottable_datetime('2014-03-27T15:09:51.000000Z'),\n",
    "                          'C1', 'Deployment 7D')\n",
    "TEMP_DEPLOYMENTS['7F'] = (utc_time_string_to_plottable_datetime('2012-12-31T23:59:59.000000Z'),\n",
    "                          utc_time_string_to_plottable_datetime('2014-11-15T00:43:14.000000Z'),\n",
    "                          'C3', 'Deployment 7F')\n",
    "TEMP_DEPLOYMENTS['7G'] = (utc_time_string_to_plottable_datetime('2014-01-01T00:00:06.000000Z'),\n",
    "                          utc_time_string_to_plottable_datetime('2016-02-09T21:04:29.000000Z'),\n",
    "                          'C4', 'Deployment 7G')\n",
    "TEMP_DEPLOYMENTS['OA'] = (utc_time_string_to_plottable_datetime('2017-09-13T23:59:13.000000Z'),\n",
    "                          utc_time_string_to_plottable_datetime('2018-11-28T01:11:14.000000Z'),\n",
    "                          'C8', 'Deployment OA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IRIS_AU_STATIONS_FILE = r\"C:\\software\\hiperseis\\seismic\\gps_corrections\\AU_irisws-fedcatalog_20190305T012747Z.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_picks = pd.read_csv(PICKS_PATH, ' ', header=0, dtype=dtype)\n",
    "len(df_raw_picks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate catalog of major regional events (mag 8+) for overlays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    df_mag8 = df_raw_picks[df_raw_picks['mag'] >= 8.0]\n",
    "    df_mag8['day'] = df_mag8['originTimestamp'].transform(datetime.datetime.utcfromtimestamp).transform(lambda x: x.strftime(\"%Y-%m-%d\"))\n",
    "    df_mag8 = df_mag8.sort_values(['day', 'originTimestamp'])\n",
    "\n",
    "    day_mag8_count = [(day, len(df_day)) for day, df_day in df_mag8.groupby('day')]\n",
    "    dates, counts = zip(*day_mag8_count)\n",
    "    mag8_dict = {'date': dates, 'counts': counts}\n",
    "    mag8_events_df = pd.DataFrame(mag8_dict, columns=['date', 'counts'])\n",
    "\n",
    "    event_count_threshold = 400\n",
    "    significant_events = mag8_events_df[mag8_events_df['counts'] >= event_count_threshold]\n",
    "    significant_events = significant_events.set_index('date')\n",
    "\n",
    "    significant_events.loc['2001-06-23', 'name'] = '2001 South Peru Earthquake'\n",
    "    significant_events.loc['2001-11-14', 'name'] = '2001 Kunlun earthquake'\n",
    "    significant_events.loc['2002-11-03', 'name'] = '2002 Denali earthquake'\n",
    "    significant_events.loc['2003-09-25', 'name'] = '2003 Tokachi-Oki earthquake'\n",
    "    significant_events.loc['2004-12-26', 'name'] = '2004 Indian Ocean earthquake and tsunami'\n",
    "    significant_events.loc['2005-03-28', 'name'] = '2005 Nias-Simeulue earthquake'\n",
    "    significant_events.loc['2009-09-29', 'name'] = '2009 Samoa earthquake and tsunami'\n",
    "    significant_events.loc['2010-02-27', 'name'] = '2010 Chile earthquake'\n",
    "    significant_events.loc['2011-03-11', 'name'] = '2011 Tohoku earthquake and tsunami'\n",
    "    significant_events.loc['2012-04-11', 'name'] = '2012 Indian Ocean earthquakes'\n",
    "    significant_events.loc['2013-02-06', 'name'] = '2013 Solomon Islands earthquakes'\n",
    "    significant_events.loc['2013-09-24', 'name'] = '2013 Balochistan earthquakes'\n",
    "    significant_events.loc['2014-04-01', 'name'] = '2014 Iquique earthquake'\n",
    "    significant_events.loc['2015-09-16', 'name'] = '2015 Illapel earthquake'\n",
    "    significant_events.loc['2016-08-24', 'name'] = '2016 Myanmar earthquake'\n",
    "\n",
    "    display(significant_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query time period for source dataset\n",
    "import obspy\n",
    "\n",
    "print(obspy.UTCDateTime(df_raw_picks['originTimestamp'].min()))\n",
    "print(obspy.UTCDateTime(df_raw_picks['originTimestamp'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_AU_mask = (df_raw_picks['net'] == 'AU')\n",
    "# min_AU_date = df_raw_picks.loc[raw_AU_mask, 'originTimestamp'].min()\n",
    "# max_AU_date = df_raw_picks.loc[raw_AU_mask, 'originTimestamp'].max()\n",
    "# obspy.UTCDateTime(min_AU_date), obspy.UTCDateTime(max_AU_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# datetime.datetime.fromtimestamp(0.0, datetime.timezone.utc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Priority order of trusted channels\n",
    "# channel_pref = ['HHZ', 'HHZ_10', 'H?Z', 'BHZ_00', 'BHZ', 'BHZ_10', 'B?Z', 'S?Z', 'SHZ', '???', '?']\n",
    "channel_pref = ['HHZ', 'HHZ_10', 'H?Z', 'BHZ_00', 'BHZ', 'BHZ_10', 'B?Z', 'S?Z', 'SHZ']\n",
    "# channel_pref = ['HHZ', 'HHZ_10', 'H?Z', 'BHZ_00', 'BHZ', 'BHZ_10', 'B?Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-BHZ channels as their picks are not considered reliable enough to use\n",
    "df_picks = df_raw_picks[df_raw_picks['cha'].isin(channel_pref)].reset_index()\n",
    "print(obspy.UTCDateTime(df_picks['originTimestamp'].min()))\n",
    "print(obspy.UTCDateTime(df_picks['originTimestamp'].max()))\n",
    "len(df_picks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unused columns for readability\n",
    "df_picks = df_picks[['#eventID', 'originTimestamp', 'mag', 'originLon', 'originLat', 'originDepthKm', 'net', 'sta', 'cha', 'pickTimestamp', 'phase', \n",
    "                     'stationLon', 'stationLat', 'az', 'baz', 'distance', 'ttResidual', 'snr', 'qualityMeasureCWT', 'qualityMeasureSlope', 'nSigma']]\n",
    "print(obspy.UTCDateTime(df_picks['originTimestamp'].min()))\n",
    "print(obspy.UTCDateTime(df_picks['originTimestamp'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    # Some select stations require custom date filters to remove singular events outside the date range of the rest of the network\n",
    "    DATE_FILTER = (\n",
    "        ('7D', pd.Timestamp(datetime.datetime(2010, 1, 1))), \n",
    "        ('7G', pd.Timestamp(datetime.datetime(2010, 1, 1)))\n",
    "    )\n",
    "    # print(DATE_FILTER)\n",
    "    before = len(df_picks)\n",
    "#     print(before)\n",
    "    for net, min_date in DATE_FILTER:\n",
    "#         print(net + \".\" + stn + \": \" + str(min_date))\n",
    "        date_mask = (df_picks['net'] == net) & (df_picks['originTimestamp'] < min_date.timestamp())\n",
    "#         print(np.sum(date_mask))\n",
    "        df_picks = df_picks[~date_mask]\n",
    "    after = len(df_picks)\n",
    "    print('Removed {} events due to timestamps'.format(before - after))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNetworkStations(df, netcode):\n",
    "    return sorted(df[df['net'] == netcode]['sta'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNetworkMean(df, netcode):\n",
    "    mean_lat = df[df['net'] == netcode]['stationLat'].mean()\n",
    "    mean_lon = df[df['net'] == netcode]['stationLon'].mean()\n",
    "    return (mean_lat, mean_lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNetworkDateRange(df, netcode):\n",
    "    mask = (df['net'] == netcode)\n",
    "    df_net = df.loc[mask]\n",
    "    min_date = df_net['originTimestamp'].min()\n",
    "    max_date = df_net['originTimestamp'].max()\n",
    "    return (obspy.UTCDateTime(min_date), obspy.UTCDateTime(max_date))\n",
    "\n",
    "def getStationDateRange(df, netcode, statcode):\n",
    "    mask = (df['net'] == netcode)\n",
    "    df_net = df.loc[mask]\n",
    "    mask = (df_net['sta'] == statcode)\n",
    "    df_sta = df_net.loc[mask]\n",
    "    min_date = df_sta['originTimestamp'].min()\n",
    "    max_date = df_sta['originTimestamp'].max()\n",
    "    return (obspy.UTCDateTime(min_date), obspy.UTCDateTime(max_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOverlappingDateRange(df, ref_station, target_network):\n",
    "    mask_ref = df[list(ref_station)].isin(ref_station).all(axis=1)\n",
    "    mask_targ = df[list(target_network)].isin(target_network).all(axis=1)\n",
    "    mask = mask_ref | mask_targ\n",
    "    if not np.any(mask):\n",
    "        return (None, None)\n",
    "    df_nets = df.loc[mask]\n",
    "    keep_events = [e for e, d in df_nets.groupby('#eventID') if np.any(d[list(ref_station)].isin(ref_station).all(axis=1)) and np.any(d[list(target_network)].isin(target_network).all(axis=1))]\n",
    "    event_mask = df_nets['#eventID'].isin(keep_events)\n",
    "    df_nets = df_nets[event_mask]\n",
    "    return (obspy.UTCDateTime(df_nets['originTimestamp'].min()), obspy.UTCDateTime(df_nets['originTimestamp'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIrisStationCodes(src_file, original_network):\n",
    "    # Get station codes listed in IRIS whose network is original_network.\n",
    "    # We need this to ensure we get complete coverage of chosen network, as it is\n",
    "    # possible some such codes appear only under other network codes such as IR, GE, etc.. in the event catalog.\n",
    "    # Returns a Pandas dataframe consisting of each station code and its mean (latitude, longitude) position.\n",
    "    df = pd.read_csv(src_file, header=0, sep='|')\n",
    "    df.columns = [c.strip() for c in df.columns.tolist()]\n",
    "    au_net_df = df.loc[(df['Network'] == original_network)]\n",
    "    au_net_df.columns = [c.strip() for c in au_net_df.columns.tolist()]\n",
    "    au_perm_stations = sorted(au_net_df['Station'].unique())\n",
    "    mean_lat = []\n",
    "    mean_lon = []\n",
    "    for sta in au_perm_stations:\n",
    "        mean_lat.append(au_net_df.loc[(au_net_df['Station'] == sta), 'Latitude'].mean())\n",
    "        std_dev = au_net_df.loc[(au_net_df['Station'] == sta), 'Latitude'].std(ddof=0)\n",
    "        assert std_dev < 1.0, \"{}: {}\".format(sta, std_dev)\n",
    "        mean_lon.append(au_net_df.loc[(au_net_df['Station'] == sta), 'Longitude'].mean())\n",
    "        std_dev = au_net_df.loc[(au_net_df['Station'] == sta), 'Longitude'].std(ddof=0)\n",
    "        assert std_dev < 1.0, \"{}: {}\".format(sta, std_dev)\n",
    "    df_dict = {'sta': au_perm_stations, 'lat': mean_lat, 'lon': mean_lon}\n",
    "    result_df = pd.DataFrame(df_dict)\n",
    "    return result_df.set_index(['sta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determineAlternateMatchingCodes(df, iris_file, original_network):\n",
    "    # Find stations from other networks in df with the same station codes, but different network codes,\n",
    "    # whose positions match the stations of the same code in the original network.\n",
    "    matching_network_stn_iris_df = getIrisStationCodes(iris_file, original_network)\n",
    "\n",
    "    mask_iris_stns = df['sta'].isin(matching_network_stn_iris_df.index)\n",
    "    mask_not_orig = (df['net'] != original_network)\n",
    "    df_orig_stns_codes = df.loc[mask_iris_stns & mask_not_orig]\n",
    "\n",
    "    # For each non-original network record, compute its distance from the known corresponding original station location\n",
    "    from obspy.geodetics import locations2degrees\n",
    "    def distToOrigStn(row, orig_df):\n",
    "        row_sta = row['sta']\n",
    "        orig_df_sta = orig_df.loc[row_sta]\n",
    "        return locations2degrees(row['stationLat'], row['stationLon'], orig_df_sta['lat'], orig_df_sta['lon'])\n",
    "    print(\"Computing distances to original network station locations...\")\n",
    "    distances_from_orig = df_orig_stns_codes.apply(lambda r: distToOrigStn(r, matching_network_stn_iris_df), axis=1)\n",
    "\n",
    "    df_orig_stns_codes_matching = df_orig_stns_codes.loc[(distances_from_orig < 1.0)]\n",
    "\n",
    "    new_codes = [(n, s) for (n, s), _ in df_orig_stns_codes_matching.groupby(['net', 'sta'])]\n",
    "    new_nets, new_stas = zip(*new_codes)\n",
    "    return new_nets, new_stas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getNetworkMean(df_picks, '7B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---\n",
    "# TARGET_NET = 'AU'\n",
    "# STN_LIST = ['KDU']\n",
    "#---\n",
    "# TARGET_NET = 'AU'\n",
    "# STN_LIST = ['WR0', 'WR1', 'WR2', 'WR3', 'WR4', 'WR5', 'WR6', 'WR7','WR8', 'WR9', 'WR10']\n",
    "#---\n",
    "# TARGET_NET = '7X'\n",
    "# STN_LIST = ['MA01', 'MA33', 'MA41', 'MA42', 'MA43', 'MA44', 'MA51', 'MA62', 'MIL7']\n",
    "#---\n",
    "# TARGET_NET = '7D'\n",
    "# STN_LIST = getNetworkStations(df_picks, TARGET_NET)\n",
    "# STN_LIST = STN_LIST[0:16] # take a subset\n",
    "#---\n",
    "# TARGET_NET = '7G'\n",
    "# STN_LIST = getNetworkStations(df_picks, TARGET_NET)\n",
    "# STN_LIST = STN_LIST[0:22] # take a 1/3 subset\n",
    "#---\n",
    "# TARGET_NET = '7B'\n",
    "# STN_LIST = getNetworkStations(df_picks, TARGET_NET)\n",
    "# STN_LIST = STN_LIST[0:16] # take a subset\n",
    "#---\n",
    "TARGET_NET = 'AU'\n",
    "STN_LIST = getNetworkStations(df_picks, TARGET_NET)\n",
    "# STN_LIST = STN_LIST[0:16] # take a subset\n",
    "#---\n",
    "# TARGET_NET = 'AU'\n",
    "# STN_LIST = ['ARMA']\n",
    "#---\n",
    "TARGET_STNS = {'net': [TARGET_NET]*len(STN_LIST), 'sta': [s for s in STN_LIST]}\n",
    "\n",
    "getNetworkDateRange(df_picks, TARGET_NET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for additional codes in non-AU networks and add them to the target stations\n",
    "new_nets, new_stas = determineAlternateMatchingCodes(df_picks, IRIS_AU_STATIONS_FILE, TARGET_NET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_non_AU = ((df_picks['net'].isin(list(new_nets))) & (df_picks['sta'].isin(list(new_stas))))\n",
    "min_non_AU_date = df_picks.loc[mask_non_AU, 'originTimestamp'].min()\n",
    "max_non_AU_date = df_picks.loc[mask_non_AU, 'originTimestamp'].max()\n",
    "obspy.UTCDateTime(min_non_AU_date), obspy.UTCDateTime(max_non_AU_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_STNS['net'].extend(list(new_nets))\n",
    "TARGET_STNS['sta'].extend(list(new_stas))\n",
    "getNetworkDateRange(df_picks, TARGET_NET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---\n",
    "# REF_NET = 'AU'\n",
    "# REF_STN = 'MTN'\n",
    "#---\n",
    "# REF_NET = 'IR'\n",
    "# REF_STN = 'WRAB'\n",
    "#---\n",
    "# REF_NET = 'AU'\n",
    "# REF_STN = 'QIS' # Doesn't have much BHZ data\n",
    "#---\n",
    "# REF_NET = 'AU'\n",
    "# REF_STN = 'ARMA'\n",
    "#---\n",
    "# REF_NET = 'AU'\n",
    "# REF_STN = 'CMSA'\n",
    "#---\n",
    "# REF_NET = 'AU'\n",
    "# REF_STN = 'QLP'\n",
    "#---\n",
    "# REF_NET = 'AU'\n",
    "# REF_STN = 'QIS' # QIS, CTA, QLP, TOO, WB2, WR0, WR2, HTT, ARMA, CMSA\n",
    "#---\n",
    "REF_NET = '7X'\n",
    "REF_STN = 'MA22' # MA51\n",
    "#---\n",
    "# REF_NET = '7D'\n",
    "# REF_STN = 'DA44' # CZ40\n",
    "#---\n",
    "REF = {'net': [REF_NET], 'sta': [REF_STN]}\n",
    "\n",
    "getStationDateRange(df_picks, REF_NET, REF_STN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(getOverlappingDateRange(df_raw_picks, REF, TARGET_STNS))\n",
    "# print(getOverlappingDateRange(df_picks, REF, TARGET_STNS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_styled_table(df):\n",
    "    # Display table with blocks of same event ID highlighted\n",
    "    df['lastEventID'] = df['#eventID'].shift(1)\n",
    "    df['lastEventID'].iloc[0] = df['#eventID'].iloc[0]\n",
    "    cols = ['#ffffff', '#e0e0ff']\n",
    "    def block_highlighter(r):\n",
    "        if r['lastEventID'] != r['#eventID']:\n",
    "            block_highlighter.current_col = (block_highlighter.current_col + 1) % len(cols)\n",
    "        return ['background-color: ' + cols[block_highlighter.current_col]]*len(r)\n",
    "    block_highlighter.current_col = 0\n",
    "    return df.style.apply(block_highlighter, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter to teleseismic events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column heading for the angular distance (degrees) between event and station\n",
    "ANG_DIST = 'distance'\n",
    "mask_tele = (df_picks[ANG_DIST] >= 30.0) & (df_picks[ANG_DIST] <= 90.0)\n",
    "df_tele = df_picks.loc[mask_tele]\n",
    "len(df_tele)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter to signal quality metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "APPLY_QUALITY_TO_REF = True\n",
    "# Remove reference station records where the SNR is too low\n",
    "min_ref_snr = 10\n",
    "# min_ref_snr = 0\n",
    "mask_snr = (df_tele['snr'] >= min_ref_snr)\n",
    "\n",
    "# Filter to constrained quality metrics\n",
    "# cwt_cutoff = 10\n",
    "# slope_cutoff = 2\n",
    "# nsigma_cutoff = 4\n",
    "cwt_cutoff = 15\n",
    "slope_cutoff = 3\n",
    "nsigma_cutoff = 4\n",
    "# cwt_cutoff = 0\n",
    "# slope_cutoff = 0\n",
    "# nsigma_cutoff = 0\n",
    "mask_cwt = (df_tele['qualityMeasureCWT'] >= cwt_cutoff)\n",
    "mask_slope = (df_tele['qualityMeasureSlope'] >= slope_cutoff)\n",
    "mask_sigma = (df_tele['nSigma'] >= nsigma_cutoff)\n",
    "\n",
    "# For events from ISC catalogs the quality metrics are zero, so we use event magnitude instead.\n",
    "min_magnitude = 5.5\n",
    "# min_magnitude = 4.0\n",
    "# min_magnitude = 0.0\n",
    "mask_zero_quality_stats = (df_tele[['snr', 'qualityMeasureCWT', 'qualityMeasureSlope', 'nSigma']] == 0).all(axis=1)\n",
    "mask_origin_mag = (df_tele['mag'] >= min_magnitude)\n",
    "\n",
    "quality_mask = (mask_snr & mask_cwt & mask_slope & mask_sigma) | (mask_zero_quality_stats & mask_origin_mag)\n",
    "if APPLY_QUALITY_TO_REF:\n",
    "    # But never apply quality mask to ref stations that have all zero quality stats, as we just can't judge quality\n",
    "    # and don't want to arbitrarily exclude them.\n",
    "    mask_ref = df_tele[list(REF)].isin(REF).all(axis=1)\n",
    "    quality_mask = (mask_zero_quality_stats & mask_ref) | quality_mask\n",
    "else:\n",
    "    # Only apply quality mask stations that are not the reference station, i.e. use all ref station events\n",
    "    # regardless of pick quality at the ref station. This gives more results, but possibly more noise.\n",
    "    mask_ref = df_tele[list(REF)].isin(REF).all(axis=1)\n",
    "    quality_mask = mask_ref | (~mask_ref & quality_mask)\n",
    "\n",
    "assert np.sum(quality_mask) > 100, 'Not enough points left after quality filtering'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qual = df_tele[quality_mask]\n",
    "# df_qual = df_tele\n",
    "len(df_qual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter to desired ref and target networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_ref = df_qual[list(REF)].isin(REF).all(axis=1)\n",
    "mask_targ = df_qual[list(TARGET_STNS)].isin(TARGET_STNS).all(axis=1)\n",
    "mask = mask_ref | mask_targ\n",
    "np.any(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nets = df_qual.loc[mask]\n",
    "len(df_nets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out events in which REF and TARGET stations are not both present\n",
    "keep_events = [e for e, d in df_nets.groupby('#eventID') if np.any(d[list(REF)].isin(REF).all(axis=1)) and\n",
    "               np.any(d[list(TARGET_STNS)].isin(TARGET_STNS).all(axis=1))]\n",
    "len(keep_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_mask = df_nets['#eventID'].isin(keep_events)\n",
    "df_nets = df_nets[event_mask]\n",
    "print(len(df_nets))\n",
    "assert len(df_nets) > 0, \"No events left to analyze!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few filtered entries\n",
    "#display_styled_table(df_nets[df_nets['#eventID'].isin(keep_events[0:5])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alias for dataset at the end of all filtering, a static name that can be used from here onwards.\n",
    "ds_final = df_nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(getOverlappingDateRange(ds_final, REF, TARGET_STNS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For each event, create column for reference traveltime residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column for entire table first\n",
    "ds_final['ttResidualRef'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_duped = []\n",
    "pbar = tqdm(total=len(ds_final), ascii=True)\n",
    "for eventid, grp in ds_final.groupby('#eventID'):\n",
    "    pbar.update(len(grp))\n",
    "    ref_mask = (grp['net'] == REF['net'][0]) & (grp['sta'] == REF['sta'][0]) # Assumes a single reference station.\n",
    "    grp_ref = grp[ref_mask]\n",
    "    if grp_ref.empty:\n",
    "        continue\n",
    "    # Choose most favourable channel\n",
    "    cha = None\n",
    "    available_cha = grp_ref['cha'].values\n",
    "    for c in channel_pref:\n",
    "        if c in available_cha:\n",
    "            cha = c\n",
    "            break\n",
    "    # We must find a channel\n",
    "    if cha is None:\n",
    "        print(\"WARNING: Channels {} are not amongst allowed channels {}\".format(available_cha, channel_pref))\n",
    "        continue\n",
    "    cha_mask = (grp_ref['cha'] == cha)\n",
    "    grp_cha = grp_ref[cha_mask]\n",
    "    tt_ref_series = grp_cha['ttResidual'].unique()\n",
    "    if len(tt_ref_series) > 1:\n",
    "#         print(\"WARNING: Multiple reference times found for event {}\\n{},\"\n",
    "#               \" choosing smallest absolute residual\".format(eventid, grp_cha))\n",
    "        ref_duped.append(grp_ref)\n",
    "        # In this case, choose the smallest reference tt residual\n",
    "        grp_cha['absTTResidual'] = np.abs(grp_cha['ttResidual'].values)\n",
    "        grp_cha = grp_cha.sort_values('absTTResidual')\n",
    "        tt_ref_series = grp_cha['ttResidual'].unique()\n",
    "    ref_time = tt_ref_series[0]\n",
    "    ds_final.loc[grp.index, 'ttResidualRef'] = ref_time\n",
    "pbar.close()\n",
    "if ref_duped:\n",
    "    ref_duped_all = pd.concat(ref_duped)\n",
    "    ref_duped_all.to_csv(\"REF_ARRIVAL_DUPES.txt\", sep=' ', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality check - each event should have only one unique reference tt residual\n",
    "assert np.all([len(df['ttResidualRef'].unique()) == 1 for e, df in ds_final.groupby('#eventID')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_final['relTtResidual'] = ds_final['ttResidual'] - ds_final['ttResidualRef']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-order columns\n",
    "ds_final = ds_final[['#eventID', 'originTimestamp', 'mag', 'originLon', 'originLat', 'originDepthKm', 'net', 'sta', 'cha', 'pickTimestamp', 'phase',\n",
    "                     'stationLon', 'stationLat', 'distance', 'snr', 'ttResidual', 'ttResidualRef', 'relTtResidual',\n",
    "                     'qualityMeasureCWT', 'qualityMeasureSlope', 'nSigma']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Sort data by event origin time\n",
    "ds_final = ds_final.sort_values(['#eventID', 'originTimestamp'])\n",
    "#display_styled_table(ds_final.iloc[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_time = ds_final.sort_values(['originTimestamp'])\n",
    "# Remove self-residuals (i.e. residual relative to oneself, which is not useful on the graph)\n",
    "ds_time = ds_time[ds_time['relTtResidual'] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandasTimestampToPlottableDatetime(data):\n",
    "    return data.transform(datetime.datetime.utcfromtimestamp).astype('datetime64[ms]').dt.to_pydatetime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add7DMarkerLines():\n",
    "    for i, x in enumerate(np.array([1.354e9, 1.357e9, 1.362e9, 1.373e9])):\n",
    "        plt.axvline(x, linestyle='--', linewidth=2, alpha=0.5, color='C'+str(i//2))\n",
    "    plt.text(1.354e9, 40, 'Drift 1', horizontalalignment='right', fontsize=18, fontstyle='italic', alpha=0.6)\n",
    "    plt.text(1.373e9, 40, 'Drift 2', horizontalalignment='left', fontsize=18, fontstyle='italic', alpha=0.6)\n",
    "\n",
    "def addEventMarkerLines():\n",
    "    time_lims = plt.xlim()\n",
    "    y_lims = plt.ylim()\n",
    "    for date, event in significant_events.iterrows():\n",
    "        event_time = pytz.utc.localize(datetime.datetime.strptime(date, \"%Y-%m-%d\"))\n",
    "        if event_time < matplotlib.dates.num2date(time_lims[0]) or event_time >= matplotlib.dates.num2date(time_lims[1]):\n",
    "            continue\n",
    "        plt.axvline(event_time, linestyle='--', linewidth=1, color='#00800080')\n",
    "        plt.text(event_time, y_lims[0] + 0.01*(y_lims[1] - y_lims[0]), event['name'], horizontalalignment='center', verticalalignment='bottom',\n",
    "                 fontsize=12, fontstyle='italic', color='#008000c0', rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting for standard rel TT residuals chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTargetNetworkRelResiduals(df, target, ref, tt_scale=60, snr_scale=(0,60), save_file=False, file_label='', annotator=None):\n",
    "    \n",
    "    def plotDataset(ds, net_code, ref_code):\n",
    "        # Sort ds rows by SNR, so that the weakest SNR points are drawn first and the high SNR point last,\n",
    "        # to make sure high SNR point are in the top rendering layer.\n",
    "        ds = ds.sort_values('snr')\n",
    "        times = pandasTimestampToPlottableDatetime(ds['originTimestamp'])\n",
    "        vals = ds[yaxis].values\n",
    "        qual = ds['snr'].values\n",
    "        min_mag = 4.0\n",
    "        mag = ds['mag'].values - min_mag\n",
    "        ylabel = 'Relative TT residual (sec)'\n",
    "        title = \"Network {} TT residual relative to {} (filtering: ref SNR$\\geq${}, CWT$\\geq${}, slope$\\geq${}, $n\\sigma\\geq{}$)\".format(\n",
    "            net_code, ref_code, str(min_ref_snr), str(cwt_cutoff), str(slope_cutoff), str(nsigma_cutoff))\n",
    "        if len(vals) > 0:\n",
    "            plt.figure(figsize=(32,9))\n",
    "            sc = plt.scatter(times, vals, c=qual, alpha=0.5, cmap='gnuplot_r', s=np.maximum(50*mag, 10), edgecolors=None, linewidths=0)\n",
    "            time_formatter = matplotlib.dates.DateFormatter(\"%Y-%m-%d\")\n",
    "            plt.axes().xaxis.set_major_formatter(time_formatter)\n",
    "            cb = plt.colorbar(sc, drawedges=False)\n",
    "            cb.set_label('Signal to noise ratio', fontsize=12)\n",
    "            plt.grid(color='#808080', linestyle=':', alpha=0.5)\n",
    "            plt.xlabel(xlabel, fontsize=14)\n",
    "            plt.ylabel(ylabel, fontsize=14)\n",
    "            plt.xticks(fontsize=14)\n",
    "            plt.yticks(fontsize=14)\n",
    "            plt.xlim(time_range)\n",
    "            plt.ylim((-tt_scale, tt_scale))\n",
    "            plt.clim(snr_scale)\n",
    "            plt.title(title, fontsize=18)\n",
    "            lgd = plt.legend(['Point size = Mag - {}, Color = SNR'.format(min_mag)], fontsize=12, loc=1)\n",
    "            plt.text(0.01, 0.96, \"Channel selection: {}\".format(channel_pref), transform=plt.gca().transAxes, fontsize=12)\n",
    "            plt.text(0.01, 0.92, \"Start date: {}\".format(str(time_range[0])), transform=plt.gca().transAxes, fontsize=12)\n",
    "            plt.text(0.01, 0.88, \"  End date: {}\".format(str(time_range[1])), transform=plt.gca().transAxes, fontsize=12)\n",
    "            if annotator is not None:\n",
    "                annotator()\n",
    "            if save_file:\n",
    "                subfolder = os.path.join(net, ref_code)\n",
    "                os.makedirs(subfolder, exist_ok=True)\n",
    "                plt_file = os.path.join(subfolder, stn_code.replace(\"*\", \"ALL\") + '_' + ylabel.replace(\" \", \"\").replace(\".*\", \"\") + file_label + \".png\")\n",
    "                plt.savefig(plt_file, dpi=150)\n",
    "    # end plotDataset\n",
    "    \n",
    "    df_times = pandasTimestampToPlottableDatetime(df['originTimestamp'])\n",
    "    time_range = (df_times.min(), df_times.max())\n",
    "    print(\" to \".join([t.strftime(\"%Y-%m-%d %H:%M:%S\") for t in time_range]))\n",
    "    yaxis='relTtResidual'\n",
    "    ref_code = \".\".join([ref['net'][0], ref['sta'][0]])\n",
    "    xlabel = 'Event Origin Timestamp'\n",
    "#     for i, stn in enumerate(target['sta']):\n",
    "#         net = target['net'][i]\n",
    "#         df_sample = df.loc[(df['net'] == net) & (df['sta'] == stn), ['#eventID', 'originTimestamp', 'mag', 'net', 'sta', yaxis, 'snr',\\\n",
    "#                                                                      'qualityMeasureCWT', 'qualityMeasureSlope', 'nSigma']]\n",
    "#         plotDataset(df_sample, net, stn, ref_code)\n",
    "        \n",
    "    if True:\n",
    "        # Remove reference station from target set before producing composite image.\n",
    "        # The reference station may not be there, but remove it if it is.\n",
    "        mask_ref = df[list(ref)].isin(ref).all(axis=1)\n",
    "        mask_targ = df[list(target)].isin(target).all(axis=1)\n",
    "        df_agg = df[(mask_targ) & (~mask_ref)]\n",
    "        plotDataset(df_agg, ','.join(np.unique(target['net'])), ref_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# display_styled_table(ds_final[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plotTargetNetworkRelResiduals(ds_final, TARGET_STNS, REF, save_file=False, plot_aggregate=True)\n",
    "\n",
    "plotTargetNetworkRelResiduals(ds_final, TARGET_STNS, REF, save_file=False, annotator=addEventMarkerLines)\n",
    "\n",
    "# plotTargetNetworkRelResiduals(ds_final, TARGET_STNS, REF, save_file=False, plot_aggregate=False, annotator=add7DMarkerLines)\n",
    "\n",
    "# plotTargetNetworkRelResiduals(ds_final[(ds_final['originTimestamp'] >= 1.35e9) & (ds_final['originTimestamp'] <= 1.38e9)],\n",
    "#                               TARGET_STNS, REF, save_file=False, plot_aggregate=False, annotator=add7DMarkerLines, file_label='(zoomed)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Repeat, focusing on only results with high SNR\n",
    "# df_high_snr = ds_final[ds_final['snr'] >= 10.0]\n",
    "# plotTargetNetworkRelResiduals(df_high_snr, TARGET_STNS, REF, file_label='(high_SNR)', timeaxis=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove REF_STN from dataset used for this plot.\n",
    "mask_targ_snr_plot = ds_final[list(TARGET_STNS)].isin(TARGET_STNS).all(axis=1)\n",
    "ds_snr_plot = ds_final[mask_targ_snr_plot]\n",
    "\n",
    "plt.figure(figsize=(32,9))\n",
    "\n",
    "COLOR_BY_DATE=True\n",
    "if COLOR_BY_DATE:\n",
    "    # Color by date\n",
    "    time_series = ds_snr_plot['originTimestamp']\n",
    "    sc = plt.scatter(ds_snr_plot['relTtResidual'], ds_snr_plot['snr'], c=time_series.values, alpha=0.5, s=30*(ds_snr_plot['mag'] - 4))\n",
    "    time_formatter = matplotlib.dates.DateFormatter(\"%Y-%m-%d\")\n",
    "    cb = plt.colorbar(sc)\n",
    "    cb.set_label(\"Event Origin Timestamp\")\n",
    "    cb_yticks = cb.ax.get_yticks()\n",
    "    cb_ytick_labels = [datetime.datetime.utcfromtimestamp(date_tick).strftime(\"%Y-%m-%d\") for date_tick in cb_yticks]\n",
    "    cb.set_ticks(cb_yticks)\n",
    "    cb.set_ticklabels(cb_ytick_labels)\n",
    "    plt.legend(['Point size = Mag - 4.0, Color = Event Timestamp'], fontsize=12)\n",
    "else:\n",
    "    # Color by station code\n",
    "    stations = sorted(list(set(ds_snr_plot['sta'].unique())))\n",
    "    # print(stations)\n",
    "    colors = ['C' + str(n%10) for n in range(len(stations))]\n",
    "    # print(colors)\n",
    "    cdict = dict(zip(stations, colors))\n",
    "    cdict[REF_STN] = '#808080ff'\n",
    "    plt.scatter(ds_snr_plot['relTtResidual'], ds_snr_plot['snr'], c=ds_snr_plot['sta'].apply(lambda x: cdict[x]).values, alpha=0.5, s=20*(ds_snr_plot['mag'] - 4))\n",
    "    plt.legend(['Point size = Mag - 4.0, Color = station ID'], fontsize=12)\n",
    "\n",
    "plt.grid(color='#80808080', linestyle=':')\n",
    "plt.ylim((0,100))\n",
    "plt.xlim((-55,55))\n",
    "plt.xlabel(\"Relative TT residual (sec)\")\n",
    "plt.ylabel('SNR')\n",
    "plt.title(\"SNR vs relative TT residual across target network {}\".format(TARGET_NET))\n",
    "plt.savefig('SNR_vs_relTtResidual_' + TARGET_NET + '_' + \".\".join([REF_NET, REF_STN]) + '.png', dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expose re-use of station codes 7D and 7G "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_7D = df_raw_picks.loc[(df_raw_picks['net'] == '7D')]\n",
    "df_raw_7G = df_raw_picks.loc[(df_raw_picks['net'] == '7G')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_7D = pandasTimestampToPlottableDatetime(df_raw_7D['originTimestamp'])\n",
    "dates_7G = pandasTimestampToPlottableDatetime(df_raw_7G['originTimestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_7D.sort()\n",
    "dates_7G.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_7D[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_7G[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dates_7D = pd.DataFrame(dates_7D, columns=['timestamp'])\n",
    "df_dates_7G = pd.DataFrame(dates_7G, columns=['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dates_7D['year'] = df_dates_7D['timestamp'].apply(lambda x: int(x.year))\n",
    "df_dates_7G['year'] = df_dates_7G['timestamp'].apply(lambda x: int(x.year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_count_7D = {y: len(g) for y, g in df_dates_7D.groupby('year')}\n",
    "event_count_7G = {y: len(g) for y, g in df_dates_7G.groupby('year')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_count_7D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_count_7G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_7D, y_7D = (event_count_7D.keys(), event_count_7D.values())\n",
    "x_7G, y_7G = (event_count_7G.keys(), event_count_7G.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_mask_7D = (df_raw_7D['originTimestamp'] < pd.Timestamp(datetime.datetime(2010, 1, 1)).timestamp())\n",
    "early_7D_stns = df_raw_7D.loc[date_mask_7D, 'sta'].unique()\n",
    "late_7D_stns = df_raw_7D.loc[~date_mask_7D, 'sta'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_mask_7G = (df_raw_7G['originTimestamp'] < pd.Timestamp(datetime.datetime(2010, 1, 1)).timestamp())\n",
    "early_7G_stns = df_raw_7G.loc[date_mask_7G, 'sta'].unique()\n",
    "late_7G_stns = df_raw_7G.loc[~date_mask_7G, 'sta'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(early_7D_stns)\n",
    "print(late_7D_stns)\n",
    "print(early_7G_stns)\n",
    "print(late_7G_stns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(x_7D, y_7D)\n",
    "plt.xlabel('Year', fontsize=16)\n",
    "plt.xticks(list(range(min(x_7D), max(x_7D)+1, 1)), fontsize=14)\n",
    "plt.ylabel('Pick count', fontsize=16)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.grid(':', color=\"#a0a0a080\")\n",
    "plt.title('Years of pick records in {} for network {}'.format('ensemble.p.txt', '7D'), fontsize=20)\n",
    "plt.text(1997, event_count_7D[1997] + 200, str(early_7D_stns), fontsize=12)\n",
    "plt.text(2013 - 1, event_count_7D[2013] + 200, str(late_7D_stns), horizontalalignment='right', verticalalignment='top', fontsize=12)\n",
    "plt.savefig('record_years_7D.png', dpi=300)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(x_7G, y_7G)\n",
    "plt.xlabel('Year', fontsize=16)\n",
    "plt.xticks(list(range(min(x_7G), max(x_7G)+1, 1)), fontsize=14)\n",
    "plt.ylabel('Pick count', fontsize=16)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.grid(':', color=\"#a0a0a080\")\n",
    "plt.title('Years of pick records in {} for network {}'.format('ensemble.p.txt', '7G'), fontsize=20)\n",
    "plt.text(2000, event_count_7G[2000] + 200, str(early_7G_stns), fontsize=12)\n",
    "plt.text(2015 - 1, event_count_7G[2015] + 200, str(late_7G_stns), horizontalalignment='right', verticalalignment='top', fontsize=12)\n",
    "plt.savefig('record_years_7G.png', dpi=300)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find extreme event for KDU\n",
    "# # mask = (ds_final['sta'] == 'KDU') & (ds_final['snr'] > 40) & (ds_final['relTtResidual'] < -25)\n",
    "# mask = (ds_final['sta'] == 'KDU') & (ds_final['snr'] > 40) & (ds_final['relTtResidual'] < -25) & (ds_final['mag'] > 5.5)\n",
    "# outlier = ds_final[mask]\n",
    "# outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# event_id = outlier['#eventID'].values[0]\n",
    "# event_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_event = ds_final[ds_final['#eventID'] == event_id]\n",
    "# display_styled_table(df_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.options.display.float_format = '{:.1f}'.format\n",
    "# print(df_event[['#eventID', 'originTimestamp', 'mag', 'originLon', 'originLat', 'originDepthKm', 'net', 'sta', 'cha', \n",
    "#                 'pickTimestamp', 'phase', 'stationLon', 'stationLat', 'distance', 'ttResidual', 'relTtResidual', 'snr']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obspy.UTCDateTime(1299832266.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getNetworkMean(df_picks, '7D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_picks.loc[(df_raw_picks['mag'] > 0), 'mag'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_picks[(df_raw_picks['net'] == 'AU') & (df_raw_picks['sta'] == 'KDU')]['originTimestamp'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float(obspy.UTCDateTime('2012-07-01'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obspy.UTCDateTime(773377184.866)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(df_raw_picks.loc[(df_raw_picks['net'] == '7D') & (df_raw_picks['originTimestamp'] < 1341100800.0)]['sta'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
